We thank the reviewers for their insightful feedback and positive comments! In particular thanks very much for reminding us about Marthi/Andre/Russell's work. 

Re evaluating against other baselines, including model-based MAXQ algorithms and Andre et al's HORDQ/HOCQ, this is an excellent suggestion and we will work to include these (if not completed in time for ICML at least in a journal version of this work). Conceptually we feel that the Bayesian approach provides some advantages over these ideas. First, our work demonstrates that the Bayesian approach can be a unifying framework within which to reason and learn about models and pseudorewards (PRs). It may be possible to extend RMAX+MAXQ to a HORDQ style decomposition, but it seems tricky (to us), whereas the Bayesian approach is elegant and simple (to us!) though of course we do not get the PAC style guarantees (yet). Second, though we agree that it seems likely that the Bayesian approach will offer little sample complexity advantages over model-based MAXQ approaches *with uniform priors*, the ability of these approaches to incorporate arbitrary priors seems like a major advantage. In particular better-than-uniform priors are likely to be available in practical scenarios involving multi-task RL. Better priors also yield a runtime speedup. Once again it is possible to imagine extending RMAX+MAXQ to incorporate such priors during exploration; this obviously brings it closer to the Bayesian approach.

There are also some specific differences between our approach and those above. Unlike RMAX+MAXQ, we do not create or maintain subtask models. Our prior experience has been that in practice subtask models are difficult to represent compactly (with DBNs and tree-CPTs) and are nontrivial to learn (directly as DBNs). We feel that it is more useful/efficient to focus on just learning primitive action models instead. With respect to Andre et al's work, as we understand it, they do not just learn PRs but external Q-functions (Q_E in their paper) which are defined for all states. This creates issues with state abstraction. In their later work (2006) they learn the conditional distribution of a parent exit given a child exit and then use it to recursively reconstruct the Q_E function as needed. They remark that this has better abstraction behavior. We on the other hand just learn PRs as parameters of MAXQ.

In spite of (or because of!) these differences, we agree that an empirical comparison is important and we will plan to do this.

Reviewer 2: The issue with the older observations is that they are noisy (i. e. they are values of suboptimal policies), however, there are enough of them that the posterior quickly becomes "confident" (low variance) around some poor value; this leads to poor exploration and low effect of updates for later (less noisy) observations (please see Dearden 1998 for a good explanation). Note that we do this only for the PR parameters and not the transition/rewards. Theoretically, we probably do not need to do this but empirically it works much better if we do. The PR priors are Gaussian-Gamma priors; we set the prior means and precisions to be equal for all exits. 

Reviewer 3: We will work on clarifying the "engineering" bits of the algorithm. We did have some of this in our original draft but cut it due to space. We agree this is important though and will try to reinstate it. We will also fix the presentation errors mentioned. Re solving new problems, this is a great question. Indeed we hope that by exploiting the synergies between HRL and BRL we can scale to large, complex problems which could be problematic for either alone. This is a key focus of our future work. Perhaps venues like the RL competition could be used to highlight large challenge problems to encourage this sort of exploration.


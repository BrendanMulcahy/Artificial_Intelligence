\renewcommand{\arraystretch}{0}
\begin{figure*}[ht]
\centering
\begin{tabular}{cc}
%[trim=left bottom right top, clip]
\includegraphics[trim=50 50 30 50, clip, scale=0.25]{exp/Taxib.pdf} & 
\includegraphics[trim=50 50 30 50, clip, scale=0.25]{exp/Taxinb.pdf} \\
\includegraphics[trim=50 50 30 50, clip, scale=0.25]{exp/Wargus3322b.pdf} & 
\includegraphics[trim=50 50 30 50, clip, scale=0.25]{exp/Wargus3322nb.pdf} \\
\end{tabular}

%\vspace{-0.3in}
\caption{Performance on {\sf Taxi-World} (top row) and {\sf Resource-collection} (bottom). The
prefix ``B-'' denotes Bayesian, ``Uninformed/Good'' denotes the
prior and ``MB'' denotes model-based. Left column: Bayesian methods, right: non-Bayesian methods, with Bayesian {\sc maxq} for reference.}\label{fig:nopr}
\vspace{-0.2in}
\end{figure*}

\section{Empirical Evaluation}
\label{sec:expts}

In this section, we evaluate our approach and test four hypotheses:
First, does incorporating model-based priors help speed up the
convergence of {\sc maxq} to the optimal policy? Second, does the task
hierarchy still matter if very good priors are available for primitive
actions? Third, how does Bayesian {\sc maxq} compare to standard
(flat) Bayesian RL? Does Bayesian RL perform better (in terms of
computational time) if a task hierarchy is available? Finally, can our
approach effectively learn PRs and policies that are
hierarchically optimal?
% \begin{figure}
% \begin{tabular}{cc}
% \hspace{-0.5in}\includegraphics[scale=0.4]{task/Taxi-Hierarchy.pdf}&
% \hspace{-0.5in}\includegraphics[scale=0.4]{task/Wargus-Hierarchy.pdf}\\
% \end{tabular}
% \caption{Task Hierarchies for {\sf Taxi-world} and {\sf Resource-collection}.}\label{fig:tasks}
% \vspace{-0.2in}
% \end{figure}

We first focus on evaluating the first three hypotheses using domains
where a zero PR results in hierarchical optimality. To
evaluate these hypotheses, we use two domains: the fickle version of
{\sf Taxi-World}~\cite{d-hrl-00} (625 states) and {\sf
Resource-collection}~\cite{mehta.icml08} (8265 states)~\footnote{Task hierarchies for all domains are available in the supplementary material.}. In {\sf
Taxi-World}, the agent controls a taxi in a grid-world that has to
pick up a passenger from a source location and drop them off at their
destination. The state variables consist of the location of the taxi
and the source and destination of the passenger. The actions available
to the agent consist of navigation actions and actions to pickup and
putdown the passenger.  The agent gets a reward of +20 upon completing
the task, a constant -1 reward for every action and a -10 penalty for
an erroneous action. Further, each navigation action has a 15\% chance
of moving in a direction orthogonal to the intended move. In the {\sf
Resource-collection} domain, the agent collects resources (gold and
wood) from a grid world map. Here the state
variables consist of the location of the agent, what the agent is
carrying, whether a goldmine or forest is adjacent to its current
location and whether a desired gold or wood quota has been met. The
actions available to the agent are to move to a specific location,
chop gold or harvest wood, and to deposit the item it is carrying (if
any).
% To make the task tractable we limit the set of possible moves to
% a set of four locations adjacent to each goldmine, forest or deposit
% location. 
As before, for each navigation action the agent has a
probability of moving to a random location with 0.3 probability. 
% Further, in this
% domain, there may be multiple goldmines and multiple forests and
% multiple locations next to each goldmine and forest, so the action
% space is large. 
In our experiments, the map contains two
goldmines and two forests, each containing two units of gold and two
units of wood, and the gold and wood quota is set to three each. The
agent gets a +50 reward when it meets the gold/wood quota, a constant
-1 reward for every action and an additional -1 for erroneous actions
(such as trying to deposit when it is not carrying anything).
% The task hierarchies  are shown in
% Figure~\ref{fig:tasks}.

% \begin{figure*}[ht]
% \begin{tabular}{cc}
% %\hspace{-1in}\includegraphics[scale=0.6]{Taxi-Hierarchy.pdf} &
% %\hspace{-2.5in}\includegraphics[scale=0.6]{Wargus-Hierarchy.pdf}\\
% %\vspace{-5.2in}
% & \multirow{2}{*}{\includegraphics[scale=0.5]{task/Hallway-Hierarchy.pdf}} \\
% \includegraphics[scale=0.4]{task/Taxi-Hierarchy.pdf} & \\
% \includegraphics[scale=0.4]{task/Wargus-Hierarchy.pdf} & \\
% \end{tabular}
% \caption{Task Hierarchies for {\sf Taxi-world} (top-left), {\sf Resource-collection} (bottom-left) and {\sf Hallway} (right)\\
% Note that in subtasks Sniff and Back of Hallway, parameter $p$ is perpendicular to $r$, $(x, y)$ is the location of agent when Back is called.}\label{fig:tasks}
% \end{figure*}


For the Bayesian methods, we use Dirichlet priors for
the transition function parameters and Normal-Gamma priors for the
reward function parameters. We use two priors: an uninformed prior,
set to approximate a uniform distribution, and a ``good'' prior where
a previously computed model posterior is used as the ``prior.''
% priors. The first case is an uninformed prior. Here the priors
%  are set to approximate a uniform distribution over model
% parameters. The second case is a ``good'' prior. In this case, we
% first run the Bayesian MAXQ approach for 1000 episodes on each
% problem, and use the estimated model posterior as the prior.
 The prior
distributions we use are conjugate to the likelihood, so we can compute the
posterior distributions in closed form.
In general, this is not necessary; more complex priors could be used
as long as we can sample from the posterior distribution.



The methods we evaluate are: (i) Flat Q, the standard
 Q-learning algorithm, (ii) {\sc maxq-0}, the standard,
 Q-learning algorithm for {\sc maxq} with no
PR, (iii) Bayesian model-based Q-learning with an
uninformed prior and (iv) a ``good'' prior, (v) Bayesian {\sc maxq}
(our proposed approach) with an uninformed prior and (vi) a
``good'' prior, and (vii) {\sc Rmaxq}~\cite{rmax-maxq}. In our
implementation, the Bayesian model-based Q-learning uses the same code
as the Bayesian {\sc maxq} algorithm, with a ``trivial'' hierarchy
consisting of the Root task with only the primitive actions as
children. For the Bayesian methods, the update frequency $k$ was set
to 50 for {\sf Taxi-World} and 25 for {\sf Resource-collection}. $Sim$
was set to 200 for Bayesian {\sc maxq} for {\sf Taxi-World} and 1000
for Bayesian model-based Q, and to 1000 for both for {\sf Resource
collection}. For {\sc rmaxq}, the threshold sample size $m$ was set to
5 following prior work~\cite{rmax-maxq}. The value iteration was
terminated either after 300 loops or when the successive difference
between iterations was less than $0.001$. The theoretical version of
{\sc rmaxq} requires updating and re-solving the model every
step. In practice for the larger problems, this is too time-consuming,
so we re-solve the models every 10 steps. This is similar to the update frequency $k$ for Bayesian {\sc maxq}. 
% We make two changes
% to the Bayesian MAXQ algorithm for efficiency: first, we use ``all
% states updating''~\cite{d-hrl-00}, where the completion function is
% updated for all states seen in the child task rather than just the
% exit. Second, in Line~\ref{line:sample} in {\sc Recompute\_value}, we
% sample a model from the posterior~\cite{Strens}. We observed that, as
% noted by others~\cite{icml2007}, the convergence behavior can be
% improved by sampling an approximately MAP model. We approximate this
% by computing the MAP model from our posterior, and then using an
% $\epsilon$-greedy hierarchical policy to select actions. 
The results are shown in Figure~\ref{fig:nopr} (episodes on x-axis). For {\sf Resource-collection}, {\sc rmaxq} did not finish 1000 episodes, so we show only the first 350 episodes.

From these results, comparing the Bayesian versions of {\sc maxq} to
standard {\sc maxq}, we observe that for {\sf Taxi-World}, the
Bayesian version converges faster to the optimal policy even with the
uninformed prior, while for {\sf Resource-collection}, the convergence
rates are similar. When a good prior is available, convergence is very
fast (almost immediate) in both domains. Thus, the availability of
model priors can help speed up convergence in many cases for HRL. We
further observe that {\sc rmaxq} converges more
slowly than {\sc maxq} or Bayesian {\sc maxq}, though it is much
better than Flat Q. This is different from prior
work~\cite{rmax-maxq}. This may be because our domains are more
stochastic than the {\sf Taxi-world} on which prior
results~\cite{rmax-maxq} were obtained. We conjecture that, 
as the environment becomes more stochastic, errors in primitive
model estimates may propagate into subtask models and hurt the
performance of this algorithm. In their analysis~\cite{rmax-maxq}, the authors noted that the error in the
transition function for a composite task is a function of the total number of terminal states in the subtask. The error is also
compounded as we move up the task hierarchy. This could be countered
by  increasing $m$, the sample size used to estimate model parameters. This
 would improve the accuracy of the primitive model, but 
 would further hurt the convergence rate of the algorithm.

Next, we compare the Bayesian {\sc maxq} approach to ``flat'' Bayesian
model-based Q learning. We note that in {\sf Taxi-World}, with uninformed
priors, though the ``flat'' method initially does worse, it soon
catches up to standard {\sc maxq} and then to Bayesian {\sc maxq}. This is
probably because in this domain, the primitive models are relatively
easy to acquire, and the task hierarchy provides no additional
leverage. For {\sf Resource-collection}, however, 
% Note that for the ``good'' prior in this task, Bayesian MAXQ
% and flat Bayesian model-based learning do about equally well. This
% indicates that in some cases the availability of a good model prior
% may reduce the need for a task hierarchy. However, this does not
% always happen, as shown in the {\sf Resource-collection} domain, where
even with a good prior, ``flat'' Bayesian model-based Q does not
converge. The difference is that in this case, the task hierarchy
encodes extra information that cannot be deduced just from the models.
In particular, the task hierarchy tells the agent that good policies
consist of gold/wood collection moves followed by deposit moves. Since
the reward structure in this domain is very sparse, it is difficult to
deduce this even if very good models are available. Taken together,
these results show that task hierarchies and model priors can be
complementary: in general, Bayesian {\sc maxq} outperforms both flat
Bayesian RL and {\sc maxq} (in speed of convergence, since here {\sc maxq} can
learn the optimal policy).

% Thus, good priors may not be sufficient to
% guarantee fast convergence---task hierarchies can encode additional
% information about good policies that help convergence even in the presence
% of good priors. However, if no such policy information is encoded by
% the hierarchy (e.g. the hierarchy allows all ``legal'' policies),
% then a good prior on the model reduces or eliminates the convergence advantage of
% HRL over flat RL.
%\begin{figure}
%\centerline{\includegraphics[scale=0.5]{task/Hallway-Hierarchy.pdf}}
%\vspace{-0.3in}\caption{Task Hierarchy for {\sf Hallway}.}\label{fig:hallway}
%\vspace{-0.2in}\end{figure}
\renewcommand{\arraystretch}{1}

\begin{wraptable}{l}{0.5\textwidth}\footnotesize
%\begin{center}
\vspace{-25pt}
\caption{Time for 500 episodes, {\sf Taxi-World}.}
\label{tab:time}
\begin{tabular}{|p{140pt}|p{30pt}|}
\hline

Method & Time (s)\\ \hline

Bayesian MaxQ, Uninformed Prior &205\\ 

Bayesian Model-based Q, Uninformed Prior &4684\\ 

Bayesian MaxQ, Good Prior &96\\ 

Bayesian Model-based Q, Good Prior &3089\\ 

Bayesian Model-based Q, Good Prior \& Comparable Simulations
&4006 \\ 
RMaxQ &41997 \\
MaxQ &2.06 \\
FlatQ &1.77 \\
\hline
\end{tabular}
%\end{center}
\vspace{-10pt}
\end{wraptable}
Next, we compare the time taken by the different approaches in our
experiments in {\sf Taxi-World} (Table~\ref{tab:time}). As expected,
the Bayesian RL approaches are significantly slower than the
non-Bayesian approaches, except {\sc rmaxq}. The runtime of {\sc
rmaxq} is slow because it uses value iteration to solve models instead
of TD-based simulation to update policies. Further, our implementation
of value iteration is not optimized; e.g. we do not use prioritized
sweeping~\cite{mooreatkeson93}. Out of the Bayesian methods, however,
the Bayesian {\sc maxq} approaches are significantly faster than the
flat Bayesian model-based approaches. This can be attributed to the
fact that for the flat case, during the simulation in {\sc
Recompute\_value}, a much larger task needs to be solved, while the
Bayesian {\sc maxq} approach is able to take into account the
structure of the hierarchy to only simulate subtasks as needed, which
ends up being much more efficient. However, we note that we allowed
the flat Bayesian model-based approach 1000 episodes of simulation as
opposed to 200 for Bayesian {\sc maxq}. Clearly this increases the
time taken for the flat cases. But at the same time, this is
necessary: the ``Comparable Simulations'' row (and curve in
Figure~\ref{fig:nopr} top left) shows that, if the simulations are
reduced to 250 episodes for this approach, the resulting values are no
longer reliable and the performance of the Bayesian flat approach
drops sharply.  Notice that while Flat Q runs faster than {\sc maxq}
(because of the additional ``bookkeeping'' overhead due to the task hierarchy),
Bayesian {\sc maxq} runs faster than Bayesian model-based Q. Thus,
taking advantage of the hierarchical task decomposition helps reduce
the computational cost of Bayesian RL.


% \begin{table*}[t]
% \caption{CPU time taken by various methods on {\sf Taxi-world}.}
% \label{tab:time}
% \begin{center}
% \begin{tabular}{| p{4cm} | l | l | l | l | l |}
% \hline
% \multirow{5}{*}{Method} 	&Tot. Time  for	&\multicolumn{2}{|c|}{}  &\multicolumn{2}{|c|}{}			\\ 
% 						&500 Episodes 	&\multicolumn{2}{|c|}{Episodes with} 	 &\multicolumn{2}{|c|}{Episodes with}			\\
% 						&(s)				&\multicolumn{2}{|c|}{5-20 Steps}	&\multicolumn{2}{|c|}{70-120 Steps} 		\\ 
% 						&				&\multicolumn{2}{|c|}{(near-optimal)}		&\multicolumn{2}{|c|}{(suboptimal)}				\\ \cline{3-6}
% 						&
%                                                 &Avg Time (ms)	&Count
%                                                 &Avg Time (ms)	&Count \\ \hline
						
% Bayesian MaxQ, Uninformed Prior	&179	&21		&81 		&332  	&39 				\\ \hline
% Bayesian Model-based Q, Uninformed Prior	&232	&162 	&211 	&851	&25 					\\ \hline
% Bayesian MaxQ, Good Prior		&14		&17 		&209 	&84 		&3 				\\ \hline
% Bayesian Model-based Q, Good Prior		&119	&172 	&316 	&- 		&0 				\\ \hline
% Bayesian Model-based Q, Good Prior \& Comparable Simulations 									&934	&- 		&0		&687	&56		\\ \hline
% MaxQ							&0.53	&- 		&0		&1.49 	&171				\\ \hline
% Flat Q							&1.15	&- 		&0		&0.37	&18			\\ \hline
% \end{tabular}
% \end{center}
% \end{table*}


Finally we evaluate how well our approach estimates
PRs. Here we use two domains: a {\sf Modified-Taxi-World}
and a {\sf Hallway} domain~\cite{d-hrl-00,parr:thesis} (4320
states). In {\sf Modified-Taxi-World}, We allow dropoffs at any one of
the four specific locations and do not provide a reward for task
termination. Thus the Navigate subtask needs a PR
(corresponding to the correct dropoff location) to learn a good
policy.  The {\sf Hallway} domain consists of a maze with a large
scale structure of hallways and intersections. The agent has
stochastic movement actions. 
%The task hierarchy is shown in
%Figure~\ref{fig:hallway}.
For these experiments, we use uninformed priors on the environment
model. The PR Gaussian-Gamma priors are set to prefer each
exit from a subtask equally. The baselines we use are: (i) Bayesian
{\sc maxq} and {\sc maxq} with fixed zero PR, (ii) Bayesian
{\sc maxq} and {\sc maxq} with fixed manually set PR, (iii)
flat Q, (iv) {\sc alispq}~\cite{alisp} and (v) {\sc maxq} with
a non-Bayesian PR update. This last method tracks
PR just as our approach; however, instead of a Bayesian
update, it updates the PR using a temporal difference
update, treating it as a simple value function. The results are shown
in Figure~\ref{fig:pr} (episodes on x-axis).

From these results, we first observe that the methods with zero
PR always do worse than those with ``proper''
PR, indicating that in these cases the recursively optimal
policy is not the hierarchically optimal policy. When a PR
is manually set, in both domain, {\sc maxq} converges to better
policies. We observe that in each case, the Bayesian {\sc maxq}
approach is able to learn a policy that is as good, starting with no
pseudo rewards; further, its convergence rates are often
better. Further, we observe that the simple TD update strategy (MaxQ
Non-Bayes PR in Figure~\ref{fig:pr}) fails in both cases---in {\sf
Modified-Taxi-World}, it is able to learn a policy that is
approximately as good as a recursively optimal policy, but in the {\sf
Hallway} domain, it fails to converge completely, indicating that this
strategy cannot generally learn PRs. Finally, we observe
that the tripartite Q-decomposition of {\sc alispq} is also able to
correctly learn hierarchically optimal policies, however, it converges
slowly compared to Bayesian {\sc maxq} or {\sc maxq} with manual
PRs. This is especially visible in the {\sf Hallway}
domain, where there are not many opportunities for state
abstraction. We believe this is likely because it is estimating entire
$Q$-functions rather than just the PRs. In a sense, it is doing
more work than is needed to capture the hierarchically optimal policy,
because an exact Q-function may not be needed to capture the
preference for the best exit, rather, a value that assigns it a
sufficiently high reward compared to the other exits would
suffice. Taken together, these results indicate that incorporating
Bayesian priors into {\sc maxq} can successfully learn PRs
from scratch and produce hierarchically optimal policies.

%Finally, we note that the Bayesian framework we propose is flexible
%enough to address {\em both} learning models and PRs in the
%{\sc maxq} setting, unlike baselines like {\sc rmaxq} (which does not
%learn PRs) or {\sc alispq} (which does not learn models).
\renewcommand{\arraystretch}{0}
\begin{figure*}[t]
\centering
\begin{tabular}{cc}
\includegraphics[trim=50 50 30 50, clip, scale=0.25]{exp/Taxi_Modified_b.pdf} & 
\includegraphics[trim=50 50 30 50, clip, scale=0.25]{exp/Taxi_Modified_nb.pdf} \\
\includegraphics[trim=50 50 30 50, clip, scale=0.25]{exp/Hallwayb.pdf} & 
\includegraphics[trim=50 50 30 50, clip, scale=0.25]{exp/Hallwaynb.pdf} \\
\end{tabular}
%\vspace{-0.3in}
\caption{Performance on {\sf Modified-Taxi-World} (top row) and {\sf Hallway} (bottom). ``B-'': Bayesian, ``PR'': Pseudo Reward. Left
column: Bayesian methods, right: non-Bayesian methods, with
Bayesian {\sc maxq} as reference. The bottom right figure has the same
legend as the top right figure.}\label{fig:pr}
\vspace{-0.2in}\end{figure*}

%%%% Hallway Domain
%Parr illustrated his work using the maze shown in Figure 14. This maze has a large-scale
%structure (as a series of hallways and intersections), and a small-scale structure (a series of
%obstacles that must be avoided in order to move through the hallways and intersections).
%In each trial, the agent starts in the top left corner, and it must move to any state in the
%bottom right corner room. The agent has the usual four primitive actions, North, South,
%East, and West. The actions are stochastic: with probability 0.8, they succeed, but with
%probability 0.1 the action will move to the left and with probability 0.1 the action will
%move to the right instead (e.g., a North action will move east with probability 0.1 and
%west with probability 0.1). If an action would collide with a wall or an obstacle, it has no
%eect.
%The maze is structured as a series of rooms, each containing a 12-by-12 block of states
%(and various obstacles). Some rooms are parts of hallways, because they are connected
%to two other rooms on opposite sides. Other rooms are intersections, where two or more
%hallways meet.

